<!DOCTYPE html>
<html class="writer-html4" lang="en" >
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>NvTK.Modules.Transformer &mdash; NvTK 0.0 documentation</title><link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  <script id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script type="text/javascript" src="../../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../../_static/doctools.js"></script>
        <script type="text/javascript" src="../../../_static/language_data.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../../index.html" class="icon icon-home"> NvTK
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <!-- Local TOC -->
              <div class="local-toc"></div>
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">NvTK</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../../index.html">Module code</a> &raquo;</li>
          <li><a href="../../NvTK.html">NvTK</a> &raquo;</li>
      <li>NvTK.Modules.Transformer</li>
      <li class="wy-breadcrumbs-aside">
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <h1>Source code for NvTK.Modules.Transformer</h1><div class="highlight"><pre>
<span></span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    @misc{https://doi.org/10.48550/arxiv.1706.03762,</span>
<span class="sd">        doi = {10.48550/ARXIV.1706.03762},</span>
<span class="sd">        url = {https://arxiv.org/abs/1706.03762},</span>
<span class="sd">        author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},</span>
<span class="sd">        keywords = {Computation and Language (cs.CL), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},</span>
<span class="sd">        title = {Attention Is All You Need},</span>
<span class="sd">        publisher = {arXiv},</span>
<span class="sd">        year = {2017},</span>
<span class="sd">        copyright = {arXiv.org perpetual, non-exclusive license}</span>
<span class="sd">    }</span>

<span class="sd">    @inproceedings{wolf-etal-2020-transformers,</span>
<span class="sd">        title = &quot;Transformers: State-of-the-Art Natural Language Processing&quot;,</span>
<span class="sd">        author = &quot;Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush&quot;,</span>
<span class="sd">        booktitle = &quot;Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations&quot;,</span>
<span class="sd">        month = oct,</span>
<span class="sd">        year = &quot;2020&quot;,</span>
<span class="sd">        address = &quot;Online&quot;,</span>
<span class="sd">        publisher = &quot;Association for Computational Linguistics&quot;,</span>
<span class="sd">        url = &quot;https://www.aclweb.org/anthology/2020.emnlp-demos.6&quot;,</span>
<span class="sd">        pages = &quot;38--45&quot;</span>
<span class="sd">    }</span>

<span class="sd">&#39;&#39;&#39;</span>

<span class="c1"># Code:   jiaqili@zju.edu</span>
<span class="c1">#         https://github.com/beiweixiaoxu/transformerencoder/blob/master/TransformerEncoder.py</span>
<span class="c1"># Note:   modified for onehot sequence input</span>

<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>

<span class="kn">from</span> <span class="nn">.BasicModule</span> <span class="kn">import</span> <span class="n">BasicModule</span>

<div class="viewcode-block" id="ScaledDotProductAttention"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.ScaledDotProductAttention">[docs]</a><span class="k">class</span> <span class="nc">ScaledDotProductAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_k</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ScaledDotProductAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="n">d_k</span>

<div class="viewcode-block" id="ScaledDotProductAttention.forward"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.ScaledDotProductAttention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
        <span class="c1"># |q| : (batch_size, n_heads, q_len, d_k), |k| : (batch_size, n_heads, k_len, d_k), |v| : (batch_size, n_heads, v_len, d_v)</span>
        <span class="c1"># |attn_mask| : (batch_size, n_heads, seq_len(=q_len), seq_len(=k_len))</span>
        <span class="n">attn_score</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="n">attn_score</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">attn_mask</span><span class="p">,</span> <span class="o">-</span><span class="mf">1e9</span><span class="p">)</span>
        <span class="c1"># |attn_score| : (batch_size, n_heads, q_len, k_len)</span>
        <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)(</span><span class="n">attn_score</span><span class="p">)</span>
        <span class="c1"># |attn_weights| : (batch_size, n_heads, q_len, k_len)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">attn_weights</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>
        <span class="c1"># |output| : (batch_size, n_heads, q_len, d_v)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span></div></div>


<div class="viewcode-block" id="MultiHeadAttention"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.MultiHeadAttention">[docs]</a><span class="k">class</span> <span class="nc">MultiHeadAttention</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MultiHeadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">=</span> <span class="n">n_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span> <span class="o">=</span> <span class="n">d_model</span> <span class="o">//</span> <span class="n">n_heads</span> <span class="o">//</span> <span class="mi">4</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">WQ</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">WK</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">WV</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_model</span> <span class="o">//</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scaled_dot_product_attn</span> <span class="o">=</span> <span class="n">ScaledDotProductAttention</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>

<div class="viewcode-block" id="MultiHeadAttention.forward"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.MultiHeadAttention.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
        <span class="c1"># |Q| : (batch_size, q_len, d_model), |K| : (batch_size, k_len, d_model), |V| : (batch_size, v_len, d_model)</span>
        <span class="c1"># |attn_mask| : (batch_size, seq_len(=q_len), seq_len(=k_len))</span>
        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">Q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">q_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WQ</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">k_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WK</span><span class="p">(</span><span class="n">K</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">v_heads</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">WV</span><span class="p">(</span><span class="n">V</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="c1"># |q_heads| : (batch_size, n_heads, q_len, d_k), |k_heads| : (batch_size, n_heads, k_len, d_k), |v_heads| : (batch_size, n_heads, v_len, d_v)</span>
        <span class="n">attn_mask</span> <span class="o">=</span> <span class="n">attn_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># |attn_mask| : (batch_size, n_heads, seq_len(=q_len), seq_len(=k_len))</span>
        <span class="n">attn</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">scaled_dot_product_attn</span><span class="p">(</span><span class="n">q_heads</span><span class="p">,</span> <span class="n">k_heads</span><span class="p">,</span> <span class="n">v_heads</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
        <span class="c1"># |attn| : (batch_size, n_heads, q_len, d_v)</span>
        <span class="c1"># |attn_weights| : (batch_size, n_heads, q_len, k_len)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_heads</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_v</span><span class="p">)</span>
        <span class="c1"># |attn| : (batch_size, q_len, n_heads * d_v)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="c1"># |output| : (batch_size, q_len, d_model)</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">attn_weights</span></div></div>


<div class="viewcode-block" id="PositionWiseFeedForwardNetwork"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.PositionWiseFeedForwardNetwork">[docs]</a><span class="k">class</span> <span class="nc">PositionWiseFeedForwardNetwork</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PositionWiseFeedForwardNetwork</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

<div class="viewcode-block" id="PositionWiseFeedForwardNetwork.forward"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.PositionWiseFeedForwardNetwork.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># |inputs| : (batch_size, seq_len, d_model)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">inputs</span><span class="p">))</span>
        <span class="c1"># |output| : (batch_size, seq_len, d_ff)</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="c1"># |output| : (batch_size, seq_len, d_model)</span>
        <span class="k">return</span> <span class="n">output</span></div></div>


<div class="viewcode-block" id="EncoderLayer"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.EncoderLayer">[docs]</a><span class="k">class</span> <span class="nc">EncoderLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">p_drop</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EncoderLayer</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mha</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span> <span class="o">=</span> <span class="n">PositionWiseFeedForwardNetwork</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p_drop</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LayerNorm</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">)</span>

<div class="viewcode-block" id="EncoderLayer.forward"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.EncoderLayer.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">):</span>
        <span class="c1"># |inputs| : (batch_size, seq_len, d_model)</span>
        <span class="c1"># |attn_mask| : (batch_size, seq_len, seq_len)</span>
        <span class="n">attn_outputs</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mha</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">)</span>
        <span class="n">attn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout1</span><span class="p">(</span><span class="n">attn_outputs</span><span class="p">)</span>
        <span class="n">attn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm1</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">attn_outputs</span><span class="p">)</span>
        <span class="c1"># |attn_outputs| : (batch_size, seq_len(=q_len), d_model)</span>
        <span class="c1"># |attn_weights| : (batch_size, n_heads, q_len, k_len)</span>
        <span class="n">ffn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ffn</span><span class="p">(</span><span class="n">attn_outputs</span><span class="p">)</span>
        <span class="n">ffn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout2</span><span class="p">(</span><span class="n">ffn_outputs</span><span class="p">)</span>
        <span class="n">ffn_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm2</span><span class="p">(</span><span class="n">attn_outputs</span> <span class="o">+</span> <span class="n">ffn_outputs</span><span class="p">)</span>
        <span class="c1"># |ffn_outputs| : (batch_size, seq_len, d_model)</span>
        <span class="k">return</span> <span class="n">ffn_outputs</span><span class="p">,</span> <span class="n">attn_weights</span></div></div>


<div class="viewcode-block" id="TransformerEncoder"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.TransformerEncoder">[docs]</a><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">BasicModule</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer Encoder in NvTK. </span>
<span class="sd">    TransformerEncoder is a stack of MultHeadAttention encoder layers.</span>

<span class="sd">    Args:</span>
<span class="sd">        vocab_size (int)    : vocabulary size (vocabulary: collection mapping token to numerical identifiers)</span>
<span class="sd">        seq_len    (int)    : input sequence length</span>
<span class="sd">        d_model    (int)    : number of expected features in the input</span>
<span class="sd">        n_layers   (int)    : number of sub-encoder-layers in the encoder</span>
<span class="sd">        n_heads    (int)    : number of heads in the multiheadattention models</span>
<span class="sd">        p_drop     (float)  : dropout value</span>
<span class="sd">        d_ff       (int)    : dimension of the feedforward network model</span>
<span class="sd">        pad_id     (int)    : pad token id</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; encoder = TransformerEncoder(vocab_size=1000, seq_len=512)</span>
<span class="sd">    &gt;&gt;&gt; inp = torch.arange(512).repeat(2, )</span>
<span class="sd">    &gt;&gt;&gt; encoder(inp)</span>
<span class="sd">    </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">d_model</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">n_layers</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">n_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">p_drop</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">d_ff</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">pad_id</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">4</span><span class="p">),</span> <span class="n">embedding</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embedding_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fix_embedding</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformerEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="c1"># word embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pad_id</span> <span class="o">=</span> <span class="n">pad_id</span>
        <span class="c1"># embedding layers</span>
        <span class="k">if</span> <span class="n">embedding</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">embedding</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span> 
        <span class="k">if</span> <span class="n">embedding_weight</span><span class="p">:</span> 
            <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">embedding_weight</span><span class="p">)</span> <span class="c1"># å¦ææè®­ç»å¥½çembè¾å¥</span>
            <span class="c1"># ä¹å¯ä»¥åæ self.emb = nn.from_pretrained(embedding_weight)</span>
        <span class="c1"># self.embedding.weight.requires_grad = False if fix_embedding else True # è®¾å®embæ¯å¦éè®­ç»æ´æ°</span>
        
        <span class="c1"># pos_embedding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sinusoid_table</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_sinusoid_table</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>  <span class="c1"># (seq_len+æºå¨å­¦ä¹ , d_model)  # ?å¼å¤´?</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sinusoid_table</span><span class="p">,</span> <span class="n">freeze</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="c1"># EncoderLayer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">([</span><span class="n">EncoderLayer</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">n_heads</span><span class="p">,</span> <span class="n">p_drop</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_layers</span><span class="p">)])</span>
        
        <span class="c1"># attention_weight</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># layers to classify</span>
        <span class="c1"># self.linear = nn.Linear(d_model, 2)  # ä¸éè¦å linearå±</span>
        <span class="c1"># self.log_softmax = F.log_softmax</span>

<div class="viewcode-block" id="TransformerEncoder.forward"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.TransformerEncoder.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="c1"># |inputs| : (batch_size, channel=4, seq_len)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="n">inputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># (batch_size, seq_len, d_model)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

        <span class="n">positions</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">inputs</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>
        <span class="c1"># |positions| : (batch_size * seq_len)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">positions</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>        
        
        <span class="n">position_pad_mask</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="c1">#inputs.eq(self.pad_id)  # position_pad_mask:padè¿çä½ç½®ä¸º1ï¼å¶å®ä½ç½®ä¸º0</span>
        <span class="n">positions</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">position_pad_mask</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1"># positions:padè¿çä½ç½®ä¸º0ï¼å¶å®ä½ç½®æåæ¥çæ å·</span>
        <span class="c1"># |positions| : (batch_size, seq_len)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">positions</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">outputs</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pos_embedding</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        <span class="c1"># |outputs| : (batch_size, seq_len, d_model)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
        
        <span class="n">attn_pad_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_attention_padding_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_id</span><span class="p">)</span>
        <span class="c1"># |attn_pad_mask| : (batch_size, seq_len, seq_len)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">attn_pad_mask</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">attn_pad_mask</span><span class="p">)</span>
        
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="c1"># |outputs| : (batch_size, seq_len, d_model)</span>
            <span class="n">outputs</span><span class="p">,</span> <span class="n">attn_weights</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">attn_pad_mask</span><span class="p">)</span>
            <span class="c1"># |attn_weights| : (batch_size, n_heads, seq_len, seq_len)</span>
            <span class="n">attention_weights</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">attn_weights</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
        <span class="c1"># |attn_weights| : (n_layers, batch_size, n_heads, seq_len, seq_len)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span> <span class="o">=</span> <span class="n">attention_weights</span> <span class="c1"># np.array(attention_weights)</span>

        <span class="c1"># |outputs| : (batch_size, seq_len, d_model)</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">outputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="n">logging</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># |outputs| : (batch_size, d_model)</span>
        <span class="c1"># outputs = self.log_softmax(self.linear(outputs),dim=-1)</span>
        <span class="c1"># |outputs| : (batch_size, 2)</span>
        <span class="k">return</span> <span class="n">outputs</span> <span class="c1">#, attention_weights</span></div>

<div class="viewcode-block" id="TransformerEncoder.get_attention_padding_mask"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.TransformerEncoder.get_attention_padding_mask">[docs]</a>    <span class="k">def</span> <span class="nf">get_attention_padding_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">pad_id</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Mask Attention Padding.</span>

<span class="sd">        Args:</span>
<span class="sd">            q   (torch.Tensor) : query tensor</span>
<span class="sd">            k   (torch.Tensor) : key tensor</span>
<span class="sd">            pad_id  (int)   : pad token id</span>

<span class="sd">        Return:</span>
<span class="sd">            attn_pad_mask (torch.BoolTensor)  :   Attention Padding Masks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">attn_pad_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="c1"># k.eq(pad_id).unsqueeze(1).repeat(1, q.size(1), 1)</span>
        <span class="c1"># |attn_pad_mask| : (batch_size, q_len, k_len)</span>
        <span class="k">return</span> <span class="n">attn_pad_mask</span></div>

<div class="viewcode-block" id="TransformerEncoder.get_sinusoid_table"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.TransformerEncoder.get_sinusoid_table">[docs]</a>    <span class="k">def</span> <span class="nf">get_sinusoid_table</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Sinusoid Position encoding table in transformer.</span>

<span class="sd">        Args:</span>
<span class="sd">            seq_len   (int) : sequence length</span>
<span class="sd">            d_model   (int) : model dimension</span>

<span class="sd">        Return:</span>
<span class="sd">            sinusoid_table (torch.FloatTensor)  :   Sinusoid Position encoding table</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">def</span> <span class="nf">get_angle</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">power</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span> <span class="o">//</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">d_model</span><span class="p">)</span>

        <span class="n">sinusoid_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">d_model</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">sinusoid_table</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">get_angle</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">sinusoid_table</span><span class="p">[</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">get_angle</span><span class="p">(</span><span class="n">pos</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">d_model</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="n">sinusoid_table</span><span class="p">)</span></div>

<div class="viewcode-block" id="TransformerEncoder.get_attention"><a class="viewcode-back" href="../../../NvTK.Modules.html#NvTK.Modules.Transformer.TransformerEncoder.get_attention">[docs]</a>    <span class="k">def</span> <span class="nf">get_attention</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get the attention weights of Transformer Encoder</span>

<span class="sd">        Return:</span>
<span class="sd">            attention_weights (torch.FloatTensor)  :    attention weights </span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># |attn_weights| : list of (n_layers, batch_size, n_heads, seq_len, seq_len)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_weights</span></div></div>


<span class="c1"># def get_transformer_attention(model, data_loader, device=torch.device(&quot;cuda&quot;)):</span>
<span class="c1">#     attention = []</span>
    
<span class="c1">#     model.eval()</span>
<span class="c1">#     for data, target in data_loader:</span>
<span class="c1">#         data, target = data.to(device), target.to(device)</span>
<span class="c1">#         pred = model(data)</span>
<span class="c1">#         batch_attention = model.Embedding.get_attention()</span>
<span class="c1">#         batch_attention = np.array([atten.cpu().data.numpy() for atten in batch_attention]).swapaxes(0,1)</span>
<span class="c1">#         attention.append(batch_attention)</span>

<span class="c1">#     attention = np.concatenate(attention, 0) # (size, n_layers, n_heads,seq_len, seq_len)</span>
<span class="c1">#     return attention</span>
</pre></div>

           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Jiaqili@zju.edu.cn.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>